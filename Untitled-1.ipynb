{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi2\n",
      "Hi da {'num_labels': 2, 'total_rows': 100, 'train_rows': 20033, 'test_rows': 8586}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torchtext.legacy.data import Field,TabularDataset,BucketIterator\n",
    "import torch,random\n",
    "import torch.optim as optim\n",
    "\n",
    "def sayhi():\n",
    "\tprint(\"hello\")\n",
    "\n",
    "def load_data(url):\n",
    "\tusecol = [\"input\",\"output\"]\n",
    "\tif not os.path.exists(url):\n",
    "\t\tprint(\"File path does not exist\")\n",
    "\telse:\n",
    "\t\tif \"csv\" in url:\n",
    "\t\t\tdf = pd.read_csv(url,encoding='utf-8',usecols=usecol)\n",
    "\t\telse:\n",
    "\t\t\tdf = pd.read_json(url,encoding='utf-8',lines=True)\n",
    "\t\t\tdf = df[usecol][:100]\n",
    "\t\treturn df\t\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self, vocab, dimension=128):\n",
    "\t\tsuper(LSTM, self).__init__()\n",
    "\t\tself.embedding = nn.Embedding(vocab, 300)\n",
    "\t\tself.dimension = dimension\n",
    "\t\tself.lstm = nn.LSTM(input_size=300,\n",
    "\t\t\t\t\t\t\thidden_size=dimension,\n",
    "\t\t\t\t\t\t\tnum_layers=1,\n",
    "\t\t\t\t\t\t\tbatch_first=True,\n",
    "\t\t\t\t\t\t\tbidirectional=True)\n",
    "\t\tself.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "\t\tself.fc = nn.Linear(2*dimension, 1)\n",
    "\n",
    "\tdef forward(self, text, text_len):\n",
    "\n",
    "\t\ttext_emb = self.embedding(text)\n",
    "\n",
    "\t\tpacked_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "\t\tpacked_output, _ = self.lstm(packed_input)\n",
    "\t\toutput, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "\t\tout_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "\t\tout_reverse = output[:, 0, self.dimension:]\n",
    "\t\tout_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "\t\ttext_fea = self.drop(out_reduced)\n",
    "\n",
    "\t\ttext_fea = self.fc(text_fea)\n",
    "\t\ttext_fea = torch.squeeze(text_fea, 1)\n",
    "\t\ttext_out = torch.sigmoid(text_fea)\n",
    "\t\treturn text_out\n",
    "\n",
    "def get_dataset_torchtext(url,split):\n",
    "\tdf = load_data(url)\n",
    "\tSTATS = {}\n",
    "\tSTATS[\"num_labels\"] = len(df[\"output\"].unique())\n",
    "\tSTATS[\"total_rows\"] = len(df)\n",
    "\tdel df\n",
    "\tprint(\"hi\")\n",
    "\tlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "\ttext_field = Field(tokenize='spacy',tokenizer_language = 'en_core_web_sm', lower=True, include_lengths=True, batch_first=True)\n",
    "\t\n",
    "\t# fields = [('output', label_field), ('input', text_field)]\n",
    "\tfields = {'input':(\"input\",text_field),\n",
    "\t\t\t'output':(\"output\",label_field)}\n",
    "\t# TabularDataset\n",
    "\ttrain_data = TabularDataset(path=url,format='JSON', fields=fields)#, skip_header=True)\n",
    "\tprint(\"hi2\")\n",
    "\ttrain_data, valid_data = train_data.split(split_ratio=0.7, random_state = random.seed(13))\n",
    "\tSTATS[\"train_rows\"] = len(train_data)\n",
    "\tSTATS[\"test_rows\"] = len(valid_data)\n",
    "\treturn STATS,train_data,text_field,valid_data\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def lstm_train(STATS,train_data,text_field,valid_data):\n",
    "\ttrain_iter = BucketIterator(train_data, batch_size=32, sort_key=lambda x: len(x.input),\n",
    "\t\t\t\t\t\t\t\tdevice=device, sort=True, sort_within_batch=True)\n",
    "\ttest_iter = BucketIterator(valid_data, batch_size=32, sort_key=lambda x: len(x.input),\n",
    "\t\t\t\t\t\t\t\tdevice=device, sort=True, sort_within_batch=True)\n",
    "\ttext_field.build_vocab(train_data, min_freq=3)\n",
    "\tprint(\"Hi da\",STATS)\n",
    "\treturn len(text_field.vocab),train_iter,test_iter\n",
    "\n",
    "url = r\"D:\\TSApy\\NoCodeMLdash\\data\\sequence\\seq_sarcasm_train.json\"\n",
    "STATS,train_data,text_field,valid_data = get_dataset_torchtext(url,80)\n",
    "vocab,train_loader,valid_loader = lstm_train(STATS,train_data,text_field,valid_data) \n",
    "model = LSTM(vocab).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pete', 'buttigieg', 'stuns', 'campaign', 'crowd', 'by', 'speaking', 'to', 'manufacturing', 'robots', 'in', 'fluent', 'binary'] \n",
      " 1 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for x in train_data:\n",
    "    print(x.input,\"\\n\",x.output,type(x.output))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18924\\568728826.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for labels, (text, text_len) in train_loader: \n",
    "    print(labels,\"\\n\", (text, text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n",
      "torch.Size([32])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    vecs,l = x.input\n",
    "    op = x.output\n",
    "    print(vecs.shape)\n",
    "    print(l.shape)\n",
    "    print(op,op.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc9e999592116e87911213ff19e1fd4226907fa5345eb81c79f42a8c6f2ccaf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
